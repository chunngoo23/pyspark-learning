{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "postal-moscow",
   "metadata": {},
   "source": [
    "# pyspark basics: loading, exploring and saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-saskatchewan",
   "metadata": {},
   "source": [
    "# The Spark Context\n",
    "An initial note about how pySpark interacts with Jupyter notebooks: two global variables are created in the startup process. The first is the spark context:\n",
    "They provide access to many of the underlying structures used by pySpark, and you may see them referred to in code throughout the tutorials alongside functions imported from pyspark.sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "binding-passage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.2 MB 20 kB/s  eta 0:00:01   |█                               | 6.2 MB 309 kB/s eta 0:10:41     |████▊                           | 30.2 MB 124 kB/s eta 0:23:22     |███████████████████▍            | 124.0 MB 155 kB/s eta 0:08:36     |███████████████████████▎        | 148.8 MB 137 kB/s eta 0:06:44\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 539 kB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: py4j, pyspark\n",
      "    Running setup.py install for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed py4j-0.10.9 pyspark-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brutal-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession # SparkSession is all\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType, FloatType, LongType, DoubleType\n",
    "from pyspark.sql.types import StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "municipal-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a spark context\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desperate-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.195:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cooperative-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiat a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tamil-removal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.195:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb940776b20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-segment",
   "metadata": {},
   "source": [
    "# Loading csv\n",
    "使用 inferSchema 虽然能够自己推断数据类型，但是推断类型需要耗费时间。特别是在大的数据集上面。但是自己指定数据类型又容易出错。所以可以在第一次加载数据的时候使用 inferSchema，然后记下所有列的数据类型，以供下次更加方便的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excited-scholar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = SparkSession.builder.appName(\"santander\").master(\"local[*]\").getOrCreate()\n",
    "df = session.read.csv(\"../pyspark-learning/boston_house_prices.csv\", header=True, inferSchema=True, sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "手動調整schema dtyps\n",
    "custom_schema = StructType([StructField('_c0', DateType(), True),\n",
    "                           StructField('_c1', StringType(), True),\n",
    "                           StructField('_c2', DoubleType(), True),\n",
    "                           StructField('_c3', DoubleType(), True),\n",
    "                           StructField('_c4', DoubleType(), True),\n",
    "                           StructField('_c5', IntegerType(), True),\n",
    "                           ...\n",
    "                           StructField('_c27', StringType(), True)])\n",
    "                          \n",
    "df = spark.read.csv(\"../nput/train.csv\", header=False, schema=custom_schema, sep=',')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-pakistan",
   "metadata": {},
   "source": [
    "One example of using `infering` and `specifying a schema` together might be with a large, unfamiliar dataset that you know you will need to load up and work with repeatedly. The first time you load it use `inferSchema`, then make note of the dtypes it assigns. Use that information to build the custom schema, so that when you load the data in the future you avoid the extra processing time necessary for infering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-butter",
   "metadata": {},
   "source": [
    "# 常見API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-organ",
   "metadata": {},
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strong-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count numbers of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charitable-metallic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('506', 'string'),\n",
       " ('13', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string'),\n",
       " ('_c6', 'string'),\n",
       " ('_c7', 'string'),\n",
       " ('_c8', 'string'),\n",
       " ('_c9', 'string'),\n",
       " ('_c10', 'string'),\n",
       " ('_c11', 'string'),\n",
       " ('_c12', 'string'),\n",
       " ('_c13', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column-by-column dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-agent",
   "metadata": {},
   "source": [
    "For each pairing (a tuple object in Python, denoted by the parentheses), \n",
    "the first entry is the column name and the second is the dtype. \n",
    "Notice that this data has no headers with it (we specified headers=False when we loaded it), \n",
    "so Spark used its default naming convention of _c0, _c1, ... _cn. We'll makes some changes to that in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wanted-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(506='CRIM', 13='ZN', _c2='INDUS', _c3='CHAS', _c4='NOX', _c5='RM', _c6='AGE', _c7='DIS', _c8='RAD', _c9='TAX', _c10='PTRATIO', _c11='B', _c12='LSTAT', _c13='MEDV'),\n",
       " Row(506='0.00632', 13='18', _c2='2.31', _c3='0', _c4='0.538', _c5='6.575', _c6='65.2', _c7='4.09', _c8='1', _c9='296', _c10='15.3', _c11='396.9', _c12='4.98', _c13='24'),\n",
       " Row(506='0.02731', 13='0', _c2='7.07', _c3='0', _c4='0.469', _c5='6.421', _c6='78.9', _c7='4.9671', _c8='2', _c9='242', _c10='17.8', _c11='396.9', _c12='9.14', _c13='21.6'),\n",
       " Row(506='0.02729', 13='0', _c2='7.07', _c3='0', _c4='0.469', _c5='7.185', _c6='61.1', _c7='4.9671', _c8='2', _c9='242', _c10='17.8', _c11='392.83', _c12='4.03', _c13='34.7'),\n",
       " Row(506='0.03237', 13='0', _c2='2.18', _c3='0', _c4='0.458', _c5='6.998', _c6='45.8', _c7='6.0622', _c8='3', _c9='222', _c10='18.7', _c11='394.63', _c12='2.94', _c13='33.4')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek at the first few rows\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-unknown",
   "metadata": {},
   "source": [
    "In the format `column_name=value` for each row. Note that the formatting above is ugly because take doesn't try to make it pretty, it just returns the row object itself. We can use show instead and that attempts to format the data better, but because there are so many columns in this case the formatting of show doesn't fit, and each line wraps down to the next. We'll use show on a subset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "partial-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "|    506|  13|  _c2| _c3|  _c4|  _c5| _c6|   _c7|_c8|_c9|   _c10|  _c11| _c12|_c13|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|     B|LSTAT|MEDV|\n",
      "|0.00632|  18| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|  24|\n",
      "|0.02731|   0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|0.02729|   0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
      "|0.03237|   0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|0.06905|   0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "|0.02985|   0| 2.18|   0|0.458| 6.43|58.7|6.0622|  3|222|   18.7|394.12| 5.21|28.7|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012|66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172|96.1|5.9505|  5|311|   15.2| 396.9|19.15|27.1|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631| 100|6.0821|  5|311|   15.2|386.63|29.93|16.5|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004|85.9|6.5921|  5|311|   15.2|386.71| 17.1|18.9|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377|94.3|6.3467|  5|311|   15.2|392.52|20.45|  15|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009|82.9|6.2267|  5|311|   15.2| 396.9|13.27|18.9|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889|  39|5.4509|  5|311|   15.2| 390.5|15.71|21.7|\n",
      "|0.62976|   0| 8.14|   0|0.538|5.949|61.8|4.7075|  4|307|     21| 396.9| 8.26|20.4|\n",
      "|0.63796|   0| 8.14|   0|0.538|6.096|84.5|4.4619|  4|307|     21|380.02|10.26|18.2|\n",
      "|0.62739|   0| 8.14|   0|0.538|5.834|56.5|4.4986|  4|307|     21|395.62| 8.47|19.9|\n",
      "|1.05393|   0| 8.14|   0|0.538|5.935|29.3|4.4986|  4|307|     21|386.85| 6.58|23.1|\n",
      "| 0.7842|   0| 8.14|   0|0.538| 5.99|81.7|4.2579|  4|307|     21|386.75|14.67|17.5|\n",
      "|0.80271|   0| 8.14|   0|0.538|5.456|36.6|3.7965|  4|307|     21|288.99|11.69|20.2|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-eligibility",
   "metadata": {},
   "source": [
    "# Selecting and Renaming Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-insertion",
   "metadata": {},
   "source": [
    "Selecting is for selecting a subpart of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "backed-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lim = df.select('_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-accordance",
   "metadata": {},
   "source": [
    "Renaming, you can rename one column at a time, or you can change column names with list or dictionary at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "peripheral-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = ['_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13']\n",
    "new_names = ['servicer_name', 'new_int_rt', 'act_endg_upb', 'loan_age', 'mths_remng', 'aj_mths_remng', 'dt_matr', 'cd_msa', 'delq_sts', 'flag_mod', 'cd_zero_bal', 'dt_zero_bal']\n",
    "for old, new in zip(old_names, new_names):\n",
    "    df_lim = df_lim.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "duplicate-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['servicer_name',\n",
       " 'new_int_rt',\n",
       " 'act_endg_upb',\n",
       " 'loan_age',\n",
       " 'mths_remng',\n",
       " 'aj_mths_remng',\n",
       " 'dt_matr',\n",
       " 'cd_msa',\n",
       " 'delq_sts',\n",
       " 'flag_mod',\n",
       " 'cd_zero_bal',\n",
       " 'dt_zero_bal']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lim.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-ordering",
   "metadata": {},
   "source": [
    "# Describe\n",
    "Now we'll describe the data. Note that describe returns a new dataframe with the information, and so must have show called after it if our goal is to view it (note the nice formatting in this case). This can be called on one or more specific columns, as we do here, or the entire dataframe by passing no columns to describe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "planned-builder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe('_c2', '_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "packed-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|               _c2|               _c3|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               507|               507|\n",
      "|   mean|11.136778656126504|0.0691699604743083|\n",
      "| stddev| 6.860352940897589|0.2539940413404101|\n",
      "|    min|              0.46|                 0|\n",
      "|    max|             INDUS|              CHAS|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('_c2', '_c3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-anger",
   "metadata": {},
   "source": [
    "## 根据某列的字段进行分组,然后做一些计算\n",
    "df_grp = new_df.groupBy('label')\n",
    "label_avg = df_grp.avg(\"label\")\n",
    "label_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-cambodia",
   "metadata": {},
   "source": [
    "However, if you actually ran the code, you probably noticed that the the code block finished nearly instantly - despite there being over 3.5 million rows of data. This is an example of lazy computing - **nothing was actually computed here**. At the moment, we're just creating a list of instructions. All pySpark did was make sure they were valid instructions. Now let's see what happens if we tell it to show us the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-trademark",
   "metadata": {},
   "source": [
    "That takes a bit longer to run, because when you executed show you asked for a dataframe to be returned to you, which meant **Spark went back and caclulated the three previous operations**. You could have done any number of intermediate steps similar to those before calling show and they all would have been **lazy operations** that finished nearly instantly, until show ran them all.\n",
    "\n",
    "Now this would just be a background peculiarity, except that we have some control over the process. If you imagine your lineage as a **straight line of instructions leading from your source data to your ouput**, we can use the **persist() method to create a point for branching**. Essentially it tells Spark \"follow the instructions to this point, then hold these results because I'm going to come back to them again.\"\n",
    "\n",
    "Let's redo the previous code block with a persist():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-differential",
   "metadata": {},
   "source": [
    "# Persist and Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-homework",
   "metadata": {},
   "source": [
    "`.persist()` 在之後是指，我到這一步之前，我把這之前做的operation存進persistent，這樣你下次就不用再跑這一步之前的步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-destruction",
   "metadata": {},
   "source": [
    "The trailing ; simply gags the output from the command. We don't need to see the summary of what we just unpersisted)\n",
    "\n",
    "Also note that cache() is essentially a synonym for persist(), except it specifies storing the checkpoint in memory for the fastest recall, while persisting allows Spark to swap some of the checkpoint to disk if necessary. Obviously cache() only works if the dataframe you are forcing it to hold is small enough that it can fit in the memory of each node, so use it with care.\n",
    "\n",
    "And finally, a bit more on groupBy. Hopefully the usage above has given you some insight into how it works. In short, groupBy is the vehicle for aggregation in a dataframe. **A groupBy object is, in itself, incomplete. So, the line in the code block where we introduced a persist() above that looks like this:**\n",
    "\n",
    "**(sub dataframe for later operation is often used for .persist()**\n",
    "\n",
    "df_grp = perf_keep.groupBy('_c2')\n",
    "\n",
    "which generates a groupBy object where the data is grouped around the unique values found in column C2, but it is just a foundation. It is like the sentence \"We are going to group our data up by the unique values found in column C2, and then...\" The next line of code contains the rest:\n",
    "\n",
    "df_avg = df_grp.avg('_c3', '_c5', '_c6', '_c12', 'New_c12')\n",
    "\n",
    "Or to finish the sentence, \"... calculate the averages for these five columns within each group.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-cooperative",
   "metadata": {},
   "source": [
    "# 寫入數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bibliographic-department",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`target`' given input columns: [13, 506, _c10, _c11, _c12, _c13, _c2, _c3, _c4, _c5, _c6, _c7, _c8, _c9];;\n'Project ['target]\n+- Relation[506#16,13#17,_c2#18,_c3#19,_c4#20,_c5#21,_c6#22,_c7#23,_c8#24,_c9#25,_c10#26,_c11#27,_c12#28,_c13#29] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8b96857e159d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/spark_write.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# the selected dataframe to write and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_described\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3://ui-spark-social-science-public/data/mycsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/pyspark-learning/test/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \"\"\"\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/pyspark-learning/test/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/pyspark-learning/test/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/pyspark-learning/test/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`target`' given input columns: [13, 506, _c10, _c11, _c12, _c13, _c2, _c3, _c4, _c5, _c6, _c7, _c8, _c9];;\n'Project ['target]\n+- Relation[506#16,13#17,_c2#18,_c3#19,_c4#20,_c5#21,_c6#22,_c7#23,_c8#24,_c9#25,_c10#26,_c11#27,_c12#28,_c13#29] csv\n"
     ]
    }
   ],
   "source": [
    "df.select('target').write.format('com.databricks.spark.csv').option(\"header\", \"true\").save(\"../input/spark_write.csv\")\n",
    "# the selected dataframe to write and save\n",
    "df_described.write.format('com.databricks.spark.csv').option(\"header\", \"true\").save('s3://ui-spark-social-science-public/data/mycsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-emerald",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
